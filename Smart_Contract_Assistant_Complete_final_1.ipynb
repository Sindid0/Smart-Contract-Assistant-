{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#2ecc71\">Smart Contract Assistant </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "| Section | Content |\n",
    "|---------|--------|\n",
    "| **1** | Configuration & API Setup |\n",
    "| **2** | Document Ingestion Pipeline (ETL) |\n",
    "| **3** | RAG Chain Logic |\n",
    "| **4** | Interactive UI & Assessment |\n",
    "\n",
    "| Component | Model | Runs |\n",
    "|-----------|-------|------|\n",
    "| **Embedding** | `sentence-transformers/all-MiniLM-L6-v2` | Locally (free) |\n",
    "| **LLM** | `meta-llama/Llama-3.1-8B-Instruct` | Cloud (HF Inference API, free tier) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#2ecc71\">Section 1: Configuration & API Setup</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages are installed and ready!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 1.1: Verify packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import importlib\n",
    "\n",
    "packages = [\n",
    "    'dotenv', 'langchain', 'langchain_community', 'langchain_huggingface',\n",
    "    'langchain_text_splitters', 'faiss', 'gradio', 'pypdf', 'docx',\n",
    "    'docx2txt', 'rich', 'pydantic', 'numpy', 'matplotlib'\n",
    "]\n",
    "missing = []\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f'\\u274c Missing packages: {missing}')\n",
    "else:\n",
    "    print('\\u2705 All packages are installed and ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Rich console initialised.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Rich console initialised.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 1.2: Rich console setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#2ecc71\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "\n",
    "pprint('\\u2705 Rich console initialised.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Embedding model loaded (all-MiniLM-L6-v2, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">-dim).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Embedding model loaded \u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113mall-MiniLM-L6-v2, \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;38;2;46;204;113m-dim\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… LLM ready: meta-llama/Llama-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">-8B-Instruct - DETAILED MODE</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… LLM ready: meta-llama/Llama-\u001b[0m\u001b[1;36m3.1\u001b[0m\u001b[1;38;2;46;204;113m-8B-Instruct - DETAILED MODE\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">   </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">max_new_tokens</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1100</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">temp</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sampling</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">=</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m   \u001b[0m\u001b[1;33mmax_new_tokens\u001b[0m\u001b[1;38;2;46;204;113m=\u001b[0m\u001b[1;36m1100\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[1;33mtemp\u001b[0m\u001b[1;38;2;46;204;113m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[1;33msampling\u001b[0m\u001b[1;38;2;46;204;113m=\u001b[0m\u001b[1;3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 1.3: FIXED LLM Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.join('..', '.env'))\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    ")\n",
    "pprint('âœ… Embedding model loaded (all-MiniLM-L6-v2, 384-dim).')\n",
    "\n",
    "repo_id = \"meta-llama/Llama-3.1-8B-Instruct\" \n",
    "\n",
    "llm_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task='text-generation',\n",
    "    max_new_tokens=1100,         \n",
    "    do_sample=True,              \n",
    "    temperature=0.2,             \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.4,\n",
    "    huggingfacehub_api_token=os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=llm_endpoint)\n",
    "pprint(f'âœ… LLM ready: {repo_id} - DETAILED MODE')\n",
    "pprint(f'   max_new_tokens=1100, temp=0.2, sampling=True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">Cosine similarity between sample queries:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mCosine similarity between sample queries:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \"What is the termination clause?...\" â†” \"When does the contract expire?...\"  â†’  0.5063\n",
      "  \"What is the termination clause?...\" â†” \"What is the penalty for breach of contra...\"  â†’  0.4608\n",
      "  \"When does the contract expire?...\" â†” \"What is the penalty for breach of contra...\"  â†’  0.4828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Embedding model produces </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">-dimensional vectors.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;38;2;46;204;113mâœ… Embedding model produces \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;38;2;46;204;113m-dimensional vectors.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 1.4: Embedding sanity check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "\n",
    "queries = [\n",
    "    \"What is the termination clause?\",\n",
    "    \"When does the contract expire?\",\n",
    "    \"What is the penalty for breach of contract?\",\n",
    "]\n",
    "\n",
    "q_embeddings = [embedding_model.embed_query(q) for q in queries]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(np.array(q_embeddings))\n",
    "\n",
    "pprint('Cosine similarity between sample queries:')\n",
    "for i in range(len(queries)):\n",
    "    for j in range(i+1, len(queries)):\n",
    "        print(f'  \"{queries[i][:40]}...\" \\u2194 \"{queries[j][:40]}...\"  \\u2192  {sim_matrix[i][j]:.4f}')\n",
    "\n",
    "pprint(f'\\n\\u2705 Embedding model produces {len(q_embeddings[0])}-dimensional vectors.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">Testing LLM with a simple question</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mTesting LLM with a simple question\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A smart contract is a self-executing program stored on a blockchain that automates the enforcement and execution of a specific agreement or set of rules between parties. Once the contract's conditions are met, the code is triggered, allowing for the transfer of assets, execution of actions, or other predetermined outcomes without the need for intermediaries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… LCEL chain works correctly!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;38;2;46;204;113mâœ… LCEL chain works correctly!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 1.5: Simple LCEL chain test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "test_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful legal assistant. Be concise.'),\n",
    "    ('human', '{input}'),\n",
    "])\n",
    "\n",
    "test_chain = test_prompt | llm | StrOutputParser()\n",
    "\n",
    "pprint('Testing LLM with a simple question...')\n",
    "result = test_chain.invoke({'input': 'What is a smart contract in 2 sentences?'})\n",
    "print(f'\\n{result}')\n",
    "pprint('\\n\\u2705 LCEL chain works correctly!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Utility functions defined: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RPrint</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">(), </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">docs2str</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">()</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Utility functions defined: \u001b[0m\u001b[1;35mRPrint\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[1;35mdocs2str\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 1.6: Utility Runnables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough 'prints, then returns' chain.\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface:\n",
    "            print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Convert a list of LangChain Document objects into a context string.\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('source', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Source: {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "pprint('\\u2705 Utility functions defined: RPrint(), docs2str()')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#2ecc71\">Section 2: Document Ingestion Pipeline (ETL)</font>\n",
    "\n",
    "Extract â†’ Transform â†’ Load pipeline: parse documents, split into chunks, embed, and store in FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">ğŸ—‘ï¸ Fresh start: data/ and vector_store/ cleared.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mğŸ—‘ï¸ Fresh start: data/ and vector_store/ cleared.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">   Data directory:    c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\data</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m   Data directory:    c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\data\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">   Vector store path: c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\vector_store</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m   Vector store path: c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\vector_store\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 2.1: Imports & paths (Fresh Start) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import os, glob, time, shutil\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "VECTOR_STORE_PATH = os.path.join('..', 'vector_store')\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "if os.path.exists(VECTOR_STORE_PATH):\n",
    "    shutil.rmtree(VECTOR_STORE_PATH)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "\n",
    "pprint('ğŸ—‘ï¸ Fresh start: data/ and vector_store/ cleared.')\n",
    "pprint(f'   Data directory:    {os.path.abspath(DATA_DIR)}')\n",
    "pprint(f'   Vector store path: {os.path.abspath(VECTOR_STORE_PATH)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_document</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… \u001b[0m\u001b[1;35mload_document\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 2.2: Document loader helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def load_document(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Load a single document file and return LangChain Document objects.\n",
    "    Supports: .pdf, .docx, .txt\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    if ext == '.pdf':\n",
    "        docs = PyPDFLoader(file_path).load()\n",
    "    elif ext == '.docx':\n",
    "        docs = Docx2txtLoader(file_path).load()\n",
    "    elif ext == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        docs = [Document(page_content=text, metadata={'source': filename})]\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file type: {ext}')\n",
    "\n",
    "    for doc in docs:\n",
    "        doc.metadata['source'] = filename\n",
    "\n",
    "    pprint(f'  Loaded {len(docs)} page(s) from \"{filename}\"')\n",
    "    return docs\n",
    "\n",
    "pprint('\\u2705 load_document() defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">Sample text (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">485</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> chars) â†’ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chunk</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">(s):</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mSample text \u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;36m485\u001b[0m\u001b[1;38;2;46;204;113m chars\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m â†’ \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;46;204;113m \u001b[0m\u001b[1;35mchunk\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113ms\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk 1 (485 chars): \"This Agreement shall commence on the Effective Date and shall continue for a per...\"\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 2.3: Text Splitter Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "sample_text = (\n",
    "    \"This Agreement shall commence on the Effective Date and shall continue \"\n",
    "    \"for a period of twelve (12) months. Either party may terminate this \"\n",
    "    \"Agreement by providing thirty (30) days written notice. In the event \"\n",
    "    \"of a material breach, the non-breaching party may terminate immediately \"\n",
    "    \"upon written notice.\\n\\n\"\n",
    "    \"The Contractor shall perform the Services described in Exhibit A \"\n",
    "    \"in a professional and workmanlike manner. All deliverables shall \"\n",
    "    \"conform to the specifications set forth in Exhibit B.\"\n",
    ")\n",
    "\n",
    "sample_chunks = splitter.split_text(sample_text)\n",
    "pprint(f'Sample text ({len(sample_text)} chars) \\u2192 {len(sample_chunks)} chunk(s):')\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    print(f'  Chunk {i+1} ({len(chunk)} chars): \"{chunk[:80]}...\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2.4: Cumulative Ingestion Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def ingest_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    ETL pipeline: Loads, splits, and MERGES into the existing FAISS index.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    docs = load_document(file_path) \n",
    "    chunks = splitter.split_documents(docs) \n",
    "\n",
    "    new_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "    if os.path.exists(os.path.join(VECTOR_STORE_PATH, 'index.faiss')):\n",
    "        existing_store = FAISS.load_local(\n",
    "            VECTOR_STORE_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        existing_store.merge_from(new_store) \n",
    "        existing_store.save_local(VECTOR_STORE_PATH)\n",
    "    else:\n",
    "        new_store.save_local(VECTOR_STORE_PATH)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    msg = f'âœ… Ingested {len(chunks)} chunks from \"{os.path.basename(file_path)}\"'\n",
    "    pprint(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No documents found in c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\data/\n",
      "  Place your PDF, DOCX, or TXT files there and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 2.5: Batch ingest all files  \n",
    "import shutil\n",
    "\n",
    "def ingest_all_files(reset=True):\n",
    "    \"\"\"\n",
    "    Scan the data directory and ingest any PDF/DOCX/TXT files found.\n",
    "    \n",
    "    Args:\n",
    "        reset: If True, clears the vector store first to avoid duplicates.\n",
    "    \"\"\"\n",
    "    global vectorstore, retriever, rag_chain\n",
    "\n",
    "    patterns = ['*.pdf', '*.docx', '*.txt']\n",
    "    files = []\n",
    "    for pattern in patterns:\n",
    "        files.extend(glob.glob(os.path.join(DATA_DIR, pattern)))\n",
    "\n",
    "    if not files:\n",
    "        print(f'\\u26a0\\ufe0f No documents found in {os.path.abspath(DATA_DIR)}/')\n",
    "        print('  Place your PDF, DOCX, or TXT files there and re-run this cell.')\n",
    "        return\n",
    "\n",
    "    if reset and os.path.exists(VECTOR_STORE_PATH):\n",
    "        shutil.rmtree(VECTOR_STORE_PATH)\n",
    "        os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "        pprint('ğŸ—‘ï¸ Old vector store cleared (fresh start).')\n",
    "\n",
    "    pprint(f'\\nğŸ“ Found {len(files)} file(s) to ingest:')\n",
    "    for f in files:\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f'  \\u2022 {os.path.basename(f)} ({size_kb:.1f} KB)')\n",
    "    print()\n",
    "\n",
    "    total_chunks = 0\n",
    "    for i, f in enumerate(files, 1):\n",
    "        pprint(f'[{i}/{len(files)}] Processing: {os.path.basename(f)}')\n",
    "        ingest_file(f)\n",
    "\n",
    "    if os.path.exists(os.path.join(VECTOR_STORE_PATH, 'index.faiss')):\n",
    "        vectorstore = FAISS.load_local(\n",
    "            VECTOR_STORE_PATH, embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        total_chunks = len(vectorstore.docstore._dict)\n",
    "        pprint(f'\\nâœ… Batch ingestion complete! Total chunks in index: {total_chunks}')\n",
    "    else:\n",
    "        pprint('\\n\\u26a0\\ufe0f No index created.')\n",
    "\n",
    "\n",
    "ingest_all_files(reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">ğŸ—‘ï¸ Vector store cleared. Re-ingest your documents.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mğŸ—‘ï¸ Vector store cleared. Re-ingest your documents.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 2.5b: Reset Vector Store (run ONLY when you want a clean start) â”€â”€â”€â”€\n",
    "def reset_vector_store():\n",
    "    \"\"\"Delete the existing FAISS index to start fresh.\"\"\"\n",
    "    import shutil\n",
    "    if os.path.exists(VECTOR_STORE_PATH):\n",
    "        shutil.rmtree(VECTOR_STORE_PATH)\n",
    "        os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "        pprint('ğŸ—‘ï¸ Vector store cleared. Re-ingest your documents.')\n",
    "    else:\n",
    "        pprint('â„¹ï¸ No vector store found to clear.')\n",
    "\n",
    "# Uncomment to reset:\n",
    "reset_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No FAISS index found yet. Ingest some documents first.\n",
      "   Place files in c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\data and run ingest_all_files().\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 2.6: Inspect the FAISS index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "faiss_index_file = os.path.join(VECTOR_STORE_PATH, 'index.faiss')\n",
    "if os.path.exists(faiss_index_file):\n",
    "    vectorstore = FAISS.load_local(\n",
    "        VECTOR_STORE_PATH,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    total_chunks = len(vectorstore.docstore._dict)\n",
    "    pprint(f'\\u2705 FAISS index loaded: {total_chunks} chunks indexed.')\n",
    "\n",
    "    test_query = 'termination clause'\n",
    "    results = vectorstore.similarity_search_with_score(test_query, k=3)\n",
    "    pprint(f'\\nTop-3 results for \"{test_query}\":')\n",
    "    for doc, score in results:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        page = doc.metadata.get('page', '?')\n",
    "        print(f'  [{score:.4f}] ({source}, p.{page}) \"{doc.page_content[:100]}...\"')\n",
    "else:\n",
    "    print('\\u26a0\\ufe0f No FAISS index found yet. Ingest some documents first.')\n",
    "    print(f'   Place files in {os.path.abspath(DATA_DIR)} and run ingest_all_files().')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#2ecc71\">Section 3: Retrieval-Augmented Generation (RAG) Chain</font>\n",
    "\n",
    "Build the RAG pipeline: retriever, QA chain, conversation memory, evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Imports OK</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Imports OK\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from operator import itemgetter\n",
    "import json\n",
    "\n",
    "pprint('\\u2705 Imports OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âš ï¸ No index found. Creating a new FAISS index</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâš ï¸ No index found. Creating a new FAISS index\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… New initial index created and saved.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… New initial index created and saved.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Retriever configured: top-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> similarity search (was </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Retriever configured: top-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;46;204;113m similarity search \u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113mwas \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.2: Load FAISS and create retriever  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "if os.path.exists(faiss_index_file):\n",
    "    # Normal case: File exists, load it\n",
    "    vectorstore = FAISS.load_local(\n",
    "        VECTOR_STORE_PATH,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    total_chunks = len(vectorstore.docstore._dict)\n",
    "    pprint(f'âœ… Vector store loaded: {total_chunks} chunks indexed.')\n",
    "else:\n",
    "    pprint('âš ï¸ No index found. Creating a new FAISS index...')\n",
    "    \n",
    "    dummy_doc = [Document(page_content=\"system_initialization\", metadata={'source': 'system'})]\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=dummy_doc,\n",
    "        embedding=embedding_model,\n",
    "    )\n",
    "    \n",
    "    os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "    vectorstore.save_local(VECTOR_STORE_PATH)\n",
    "    pprint('âœ… New initial index created and saved.')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.2:Retriever - More chunks for detailed answers\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# âœ… CHANGED: k=8 instead of k=5 for more context\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 8},  # âœ… INCREASED from 5\n",
    ")\n",
    "\n",
    "pprint('âœ… Retriever configured: top-8 similarity search (was 5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… RAG prompt FIXED - Now demands detailed answers</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… RAG prompt FIXED - Now demands detailed answers\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.3:  RAG Prompt - Demands Detailed Answers\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a professional legal assistant specialized in contract analysis.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. **Provide DETAILED, COMPREHENSIVE answers** - at least 200-300 words\n",
    "2. **Use ALL relevant information** from the context\n",
    "3. **Structure your answer** with numbered points or bullet points\n",
    "4. **Include specific details**: amounts, dates, percentages, conditions\n",
    "5. **Cite sources** at the end: [Source: filename, Page: X]\n",
    "6. **Explain thoroughly** - never summarize, always explain in full detail\n",
    "7. **If asked about terms/clauses, list EACH ONE separately with full explanation**\n",
    "8. For Arabic questions, respond in Arabic with the same level of detail\n",
    "\n",
    "If context is insufficient, say: \"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©\" (in Arabic) or \"I don't have enough information\" (in English).\n",
    "\n",
    "---\n",
    "RETRIEVED CONTEXT:\n",
    "{context}\n",
    "---\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', SYSTEM_PROMPT),\n",
    "    ('human', '{input}'),\n",
    "])\n",
    "\n",
    "pprint('âœ… RAG prompt FIXED - Now demands detailed answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… RAG chain built (grouped context).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… RAG chain built \u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113mgrouped context\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.5: Advanced LCEL RAG Chain (FIXED) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from collections import defaultdict\n",
    "\n",
    "def format_context(docs):\n",
    "    \"\"\"Format retrieved documents, GROUPED by source file.\"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        grouped[source].append(doc)\n",
    "\n",
    "    parts = []\n",
    "    for source, source_docs in grouped.items():\n",
    "        parts.append(f'\\n{\"=\"*50}')\n",
    "        parts.append(f'ğŸ“„ DOCUMENT: {source}')\n",
    "        parts.append(f'{\"=\"*50}')\n",
    "        for doc in source_docs:\n",
    "            page = doc.metadata.get('page', '?')\n",
    "            parts.append(f'[Page {page}]:\\n{doc.page_content}')\n",
    "    \n",
    "    return '\\n\\n'.join(parts)\n",
    "\n",
    "\n",
    "# âœ… Dynamic retriever\n",
    "retrieval_chain = (\n",
    "    {'input': lambda x: x['input'] if isinstance(x, dict) else x}\n",
    "    | RunnableAssign({\n",
    "        'context_docs': (\n",
    "            itemgetter('input')\n",
    "            | RunnableLambda(lambda q: retriever.invoke(q))\n",
    "            | LongContextReorder().transform_documents\n",
    "        )\n",
    "    })\n",
    ")\n",
    "\n",
    "generation_chain = (\n",
    "    RunnableAssign({\n",
    "        'context': lambda d: format_context(d['context_docs'])\n",
    "    })\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain = retrieval_chain | RunnableAssign({'answer': generation_chain})\n",
    "\n",
    "pprint('\\u2705 RAG chain built (grouped context).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ask_question</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… \u001b[0m\u001b[1;35mask_question\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.6: ask_question helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def ask_question(query: str):\n",
    "    \"\"\"\n",
    "    Send a question through the RAG chain.\n",
    "    Returns (answer, source_pages, context_docs).\n",
    "    \"\"\"\n",
    "    response = rag_chain.invoke({'input': query})\n",
    "    answer = response['answer']\n",
    "    docs = response.get('context_docs', [])\n",
    "\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        page = doc.metadata.get('page', '?')\n",
    "        sources.append(f'{source} (p.{page})')\n",
    "    unique_sources = list(dict.fromkeys(sources))\n",
    "\n",
    "    return answer, unique_sources, docs\n",
    "\n",
    "\n",
    "pprint('\\u2705 ask_question() defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… ContractKnowledge model defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… ContractKnowledge model defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">   Fields: [</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_name'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'parties'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'key_dates'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'key_clauses'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'financial_terms'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'unresolved'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m   Fields: \u001b[0m\u001b[1;38;2;46;204;113m[\u001b[0m\u001b[32m'document_name'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'parties'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'key_dates'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'key_clauses'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'financial_terms'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'summary'\u001b[0m\u001b[1;38;2;46;204;113m, \u001b[0m\u001b[32m'unresolved'\u001b[0m\u001b[1;38;2;46;204;113m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.7: Pydantic Knowledge Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "class ContractKnowledge(BaseModel):\n",
    "    \"\"\"Running knowledge base for the current contract analysis session.\"\"\"\n",
    "    document_name: str = Field('unknown', description='Name of the uploaded document')\n",
    "    parties: List[str] = Field([], description='Parties involved in the contract')\n",
    "    key_dates: Dict[str, str] = Field({}, description='Important dates')\n",
    "    key_clauses: List[str] = Field([], description='Key clauses identified')\n",
    "    financial_terms: Dict[str, str] = Field({}, description='Payment amounts, fees, penalties')\n",
    "    summary: str = Field('No summary yet.', description='Running summary of findings')\n",
    "    unresolved: List[str] = Field([], description='Questions that could not be answered')\n",
    "\n",
    "\n",
    "session_knowledge = ContractKnowledge()\n",
    "conversation_history = []\n",
    "\n",
    "pprint('\\u2705 ContractKnowledge model defined.')\n",
    "pprint(f'   Fields: {list(ContractKnowledge.model_fields.keys())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ask_with_history</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… \u001b[0m\u001b[1;35mask_with_history\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.8: Conversational ask with history â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def ask_with_history(query: str) -> str:\n",
    "    \"\"\"Ask a question with conversation history context.\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    answer, sources, docs = ask_question(query)\n",
    "\n",
    "    conversation_history.append({'role': 'user', 'content': query})\n",
    "    conversation_history.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "    sources_str = ', '.join(sources) if sources else 'No sources'\n",
    "    formatted = f'{answer}\\n\\n\\U0001f4c4 Sources: {sources_str}'\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "pprint('\\u2705 ask_with_history() defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… eval_prompt and eval_chain defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… eval_prompt and eval_chain defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 3.9: RAG Evaluation Chain (LLM-as-a-Judge) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "EVAL_PROMPT = \"\"\"You are an expert evaluator for RAG systems.\n",
    "Given a QUESTION, the RETRIEVED CONTEXT, and the GENERATED ANSWER,\n",
    "evaluate the answer on three criteria (each scored 1-5):\n",
    "\n",
    "1. **Relevance**: Does the answer address the question? (1=irrelevant, 5=perfectly relevant)\n",
    "2. **Groundedness**: Is the answer supported by the context? (1=hallucinated, 5=fully grounded)\n",
    "3. **Completeness**: Does it cover all aspects? (1=incomplete, 5=comprehensive)\n",
    "\n",
    "Respond ONLY with valid JSON in this exact format:\n",
    "{{\n",
    "    \"relevance\": <1-5>,\n",
    "    \"groundedness\": <1-5>,\n",
    "    \"completeness\": <1-5>,\n",
    "    \"overall\": <1-5>,\n",
    "    \"reasoning\": \"<brief explanation>\"\n",
    "}}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RETRIEVED CONTEXT:\n",
    "{context}\n",
    "\n",
    "GENERATED ANSWER:\n",
    "{answer}\"\"\"\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a precise RAG evaluator. Output ONLY valid JSON.'),\n",
    "    ('human', EVAL_PROMPT),\n",
    "])\n",
    "\n",
    "eval_chain = eval_prompt | llm | StrOutputParser()\n",
    "\n",
    "pprint('âœ… eval_prompt and eval_chain defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">evaluate_answer</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… \u001b[0m\u001b[1;35mevaluate_answer\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_answer(question: str, answer: str, context_docs: list) -> dict:\n",
    "    \"\"\"Evaluate a RAG answer using LLM-as-a-Judge.\"\"\"\n",
    "    context_str = format_context(context_docs)\n",
    "    \n",
    "    clean_answer = clean_markdown(answer) if 'clean_markdown' in globals() else answer\n",
    "    \n",
    "    result = eval_chain.invoke({\n",
    "        'question': question,\n",
    "        'context': context_str,\n",
    "        'answer': clean_answer,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        clean = result.strip()\n",
    "        if clean.startswith('```'):\n",
    "            clean = clean.split('\\n', 1)[1].rsplit('```', 1)[0]\n",
    "        return json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        return {'raw_response': result, 'error': 'Failed to parse JSON'}\n",
    "\n",
    "pprint('âœ… evaluate_answer() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"#2ecc71\">Section 4: Interactive UI & Assessment</font>\n",
    "\n",
    "Gradio web interface with Upload, Chat, and Analysis tabs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… UI module imports OK</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… UI module imports OK\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 4.1: Imports & Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import gradio as gr\n",
    "import shutil, time\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "\n",
    "session_stats = {\n",
    "    'documents_uploaded': [],\n",
    "    'total_chunks': 0,\n",
    "    'questions_asked': 0,\n",
    "    'session_start': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "\n",
    "pprint('\\u2705 UI module imports OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Multi-file </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">process_upload</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Multi-file \u001b[0m\u001b[1;35mprocess_upload\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 4.2: Automatic Multi-File Cumulative Upload handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def process_upload(files):\n",
    "    \"\"\"\n",
    "    Handles multiple files automatically when uploaded.\n",
    "    New files are merged into the existing FAISS index.\n",
    "    \"\"\"\n",
    "    global vectorstore, retriever, retrieval_chain, rag_chain, session_stats\n",
    "\n",
    "    if files is None or len(files) == 0:\n",
    "        return 'âš ï¸ No files detected.'\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        new_filenames = []\n",
    "\n",
    "        for file_obj in files:\n",
    "            if hasattr(file_obj, 'name'):\n",
    "                filename = os.path.basename(file_obj.name)\n",
    "                src = file_obj.name\n",
    "            else:\n",
    "                filename = os.path.basename(file_obj)\n",
    "                src = file_obj\n",
    "\n",
    "            dest = os.path.join(DATA_DIR, filename)\n",
    "            import shutil\n",
    "            shutil.copy(src, dest)\n",
    "            ingest_file(dest)\n",
    "            new_filenames.append(filename)\n",
    "\n",
    "        # âœ… Reload vectorstore\n",
    "        vectorstore = FAISS.load_local(\n",
    "            VECTOR_STORE_PATH, embedding_model,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "        # âœ… Rebuild retriever\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type='similarity', search_kwargs={'k': 8},\n",
    "        )\n",
    "\n",
    "        # âœ… Rebuild ENTIRE chain (this is the fix!)\n",
    "        retrieval_chain = (\n",
    "            {'input': lambda x: x['input'] if isinstance(x, dict) else x}\n",
    "            | RunnableAssign({\n",
    "                'context_docs': (\n",
    "                    itemgetter('input')\n",
    "                    | retriever\n",
    "                    | LongContextReorder().transform_documents\n",
    "                )\n",
    "            })\n",
    "        )\n",
    "\n",
    "        generation_chain = (\n",
    "            RunnableAssign({'context': lambda d: format_context(d['context_docs'])})\n",
    "            | rag_prompt | llm | StrOutputParser()\n",
    "        )\n",
    "        rag_chain = retrieval_chain | RunnableAssign({'answer': generation_chain})\n",
    "\n",
    "        # Update stats\n",
    "        for name in new_filenames:\n",
    "            if name not in session_stats['documents_uploaded']:\n",
    "                session_stats['documents_uploaded'].append(name)\n",
    "\n",
    "        current_total_chunks = len(vectorstore.docstore._dict)\n",
    "        session_stats['total_chunks'] = current_total_chunks\n",
    "\n",
    "        return f'âœ… Ingested {len(new_filenames)} file(s). Total chunks: {current_total_chunks}'\n",
    "\n",
    "    except Exception as e:\n",
    "        return f'âŒ Ingestion Error: {str(e)}'\n",
    "\n",
    "pprint('âœ… Multi-file process_upload() defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat_fn</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">() defined with &lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">think</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">&gt; tag removal.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… \u001b[0m\u001b[1;35mchat_fn\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m defined with \u001b[0m\u001b[1;38;2;46;204;113m<\u001b[0m\u001b[1;95mthink\u001b[0m\u001b[1;38;2;46;204;113m>\u001b[0m\u001b[1;38;2;46;204;113m tag removal.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 4.3: Chat handler (With DeepSeek Formatting Fixes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re\n",
    "\n",
    "def clean_markdown(text):\n",
    "    \"\"\"\n",
    "    Helper to fix common markdown formatting errors and remove internal \n",
    "    Chain-of-Thought logs (<think> tags).\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    \n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "   \n",
    "    cleaned = re.sub(r'([^\\n])\\s*\\n\\*\\s', r'\\1\\n\\n* ', cleaned)\n",
    "    \n",
    "    cleaned = re.sub(r'([^\\n])\\s*(#{1,6}\\s)', r'\\1\\n\\n\\2', cleaned)\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Gradio chat callback with RAG and source citations.\"\"\"\n",
    "    global session_stats\n",
    "\n",
    "    if not message or not message.strip():\n",
    "        return 'âš ï¸ Please type a question about your uploaded document(s).'\n",
    "\n",
    "    faiss_index_file = os.path.join(VECTOR_STORE_PATH, 'index.faiss')\n",
    "    if not os.path.exists(faiss_index_file):\n",
    "        return (\n",
    "            'âš ï¸ No documents indexed yet!\\n'\n",
    "            'Please go to the \"Upload Document\" tab and upload a PDF or DOCX file first.'\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        answer, sources, docs = ask_question(message)\n",
    "        session_stats['questions_asked'] += 1\n",
    "\n",
    "        formatted_answer = clean_markdown(answer)\n",
    "\n",
    "        if sources:\n",
    "            sources_list = \"\\n\".join([f\"- {s}\" for s in sources])\n",
    "            response = f\"{formatted_answer}\\n\\n---\\n### ğŸ“„ Sources Used:\\n{sources_list}\"\n",
    "        else:\n",
    "            response = f\"{formatted_answer}\\n\\n---\\n*No specific sources cited.*\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f'âŒ Error processing question: {str(e)}'\n",
    "\n",
    "pprint('âœ… chat_fn() defined with <think> tag removal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard functions defined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# --- 4.4: Dashboard functions ------------------------------------------\n",
    "\n",
    "def get_session_stats():\n",
    "    \"\"\"Return formatted session statistics.\"\"\"\n",
    "    # Handle the case where no documents are uploaded yet\n",
    "    docs_list = '\\n'.join(f'  {d}' for d in session_stats.get('documents_uploaded', [])) or '  (none)'\n",
    "    \n",
    "    # Properly closed f-string block\n",
    "    stats = (\n",
    "        f\"SESSION STATISTICS\\n\"\n",
    "        f\"Session started:  {session_stats.get('session_start', 'N/A')}\\n\"\n",
    "        f\"Documents uploaded:\\n{docs_list}\\n\"\n",
    "        f\"Total chunks:     {session_stats.get('total_chunks', 0)}\\n\"\n",
    "        f\"Questions asked:  {session_stats.get('questions_asked', 0)}\\n\"\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "\n",
    "def run_sample_evaluation():\n",
    "    \"\"\"Run a quick evaluation on sample questions to test RAG performance.\"\"\"\n",
    "    faiss_index_file = os.path.join(VECTOR_STORE_PATH, 'index.faiss')\n",
    "    \n",
    "    if not os.path.exists(faiss_index_file):\n",
    "        return 'Upload a document first to run evaluation.'\n",
    "\n",
    "    try:\n",
    "        sample_questions = [\n",
    "            'What are the main terms of this contract?',\n",
    "            'What is the termination clause?',\n",
    "            'What are the payment terms?',\n",
    "        ]\n",
    "\n",
    "        results = []\n",
    "        for q in sample_questions:\n",
    "            answer, sources, docs = ask_question(q)\n",
    "            eval_result = evaluate_answer(q, answer, docs)\n",
    "\n",
    "            if 'error' not in eval_result:\n",
    "                results.append({\n",
    "                    'question': q,\n",
    "                    'relevance': eval_result.get('relevance', '?'),\n",
    "                    'groundedness': eval_result.get('groundedness', '?'),\n",
    "                    'completeness': eval_result.get('completeness', '?'),\n",
    "                    'overall': eval_result.get('overall', '?'),\n",
    "                })\n",
    "            else:\n",
    "                results.append({'question': q, 'error': eval_result.get('error', 'Unknown')})\n",
    "\n",
    "        report = 'RAG EVALUATION REPORT\\n' + '=' * 50 + '\\n\\n'\n",
    "        for r in results:\n",
    "            report += f\"Q: {r['question']}\\n\"\n",
    "            if 'error' in r:\n",
    "                report += f\"  Error: {r['error']}\\n\"\n",
    "            else:\n",
    "                report += f\"  Relevance:    {r['relevance']}/5\\n\"\n",
    "                report += f\"  Groundedness: {r['groundedness']}/5\\n\"\n",
    "                report += f\"  Completeness: {r['completeness']}/5\\n\"\n",
    "                report += f\"  Overall:      {r['overall']}/5\\n\"\n",
    "            report += '\\n'\n",
    "\n",
    "        # Calculate average score for valid numerical results\n",
    "        valid = [r for r in results if 'overall' in r and isinstance(r['overall'], (int, float))]\n",
    "        if valid:\n",
    "            avg = sum(r['overall'] for r in valid) / len(valid)\n",
    "            report += f'{\"-\" * 50}\\n'\n",
    "            report += f'â­ Average Overall Score: {avg:.1f}/5\\n'\n",
    "\n",
    "        return report\n",
    "\n",
    "    except Exception as e:\n",
    "        return f'Evaluation error: {str(e)}'\n",
    "\n",
    "\n",
    "def export_knowledge():\n",
    "    \"\"\"Export the session knowledge base and history as a JSON string.\"\"\"\n",
    "    data = {\n",
    "        'session_stats': session_stats,\n",
    "        # model_dump() assumes session_knowledge is a Pydantic model\n",
    "        'knowledge_base': session_knowledge.model_dump() if session_knowledge else {},\n",
    "        'conversation_history': conversation_history,\n",
    "    }\n",
    "    return json.dumps(data, indent=2, default=str)\n",
    "\n",
    "\n",
    "print('Dashboard functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Wrappers </span><span style=\"color: #008000; text-decoration-color: #008000\">'streaming_chat'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> and </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_ingest'</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> are now defined.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Wrappers \u001b[0m\u001b[32m'streaming_chat'\u001b[0m\u001b[1;38;2;46;204;113m and \u001b[0m\u001b[32m'auto_ingest'\u001b[0m\u001b[1;38;2;46;204;113m are now defined.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€ 4.4: UI Wrappers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def streaming_chat(message, history):\n",
    "    \"\"\"Adapts chat_fn for the Gradio streaming interface.\"\"\"\n",
    "    response = chat_fn(message, history) \n",
    "    yield response\n",
    "\n",
    "def auto_ingest_wrapper(files):\n",
    "    \"\"\"Handles the 'change' trigger for automatic ingestion.\"\"\"\n",
    "    if files is None:\n",
    "        return \"Waiting for files...\"\n",
    "    return process_upload(files)\n",
    "\n",
    "pprint(\"âœ… Wrappers 'streaming_chat' and 'auto_ingest' are now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ Vector store cleared successfully!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Reset Vector Store (Run ONCE to fix duplicate/wrong retrievals) â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "if os.path.exists(VECTOR_STORE_PATH):\n",
    "    shutil.rmtree(VECTOR_STORE_PATH)\n",
    "    os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "    print('ğŸ—‘ï¸ Vector store cleared successfully!')\n",
    "else:\n",
    "    print('â„¹ï¸ No vector store found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No documents found in c:\\Users\\Computec\\Desktop\\ai_assisstant_contract\\Smart_Contract_Assistant\\data/\n",
      "  Place your PDF, DOCX, or TXT files there and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Re-ingest all documents cleanly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ingest_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… UI Fixed: Multiple files can be selected and ingested automatically.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 4.5: Fixed UI: Multi-file selection + Automatic Ingestion -----------\n",
    "\n",
    "custom_css = \"\"\"\n",
    ".gradio-container { max-width: 1200px !important; margin: auto !important; }\n",
    ".main-header { text-align: center; margin-bottom: 20px; padding: 10px; border-bottom: 1px solid #e0e0e0; }\n",
    ".upload-zone { background: #f8f9fa; border: 2px dashed #2ecc71 !important; border-radius: 10px !important; }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(title='Smart Contract AI') as app:\n",
    "\n",
    "    with gr.Group(elem_classes=\"main-header\"):\n",
    "        gr.Markdown(\"# ğŸ“œ Smart Contract Assistant\")\n",
    "        gr.Markdown(\"### Multi-file upload is now enabled. Ingestion starts automatically.\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # TAB 1: Chat & Auto-Upload\n",
    "        with gr.Tab(\"ğŸ’¬ Chat & Upload\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### ğŸ“¤ Document Upload\")\n",
    "                    file_input = gr.File(\n",
    "                        label=\"Select multiple files (PDF/DOCX)\",\n",
    "                        file_types=['.pdf', '.docx', '.txt'],\n",
    "                        file_count=\"multiple\", # Enables multi-file selection\n",
    "                        type='filepath',\n",
    "                        elem_classes=\"upload-zone\"\n",
    "                    )\n",
    "                    \n",
    "                    status_box = gr.Textbox(\n",
    "                        label=\"Ingestion Status\",\n",
    "                        placeholder=\"Automatic ingestion will start after selection...\",\n",
    "                        interactive=False,\n",
    "                        lines=4\n",
    "                    )\n",
    "                    \n",
    "                    # TRIGGER: Auto-ingest when files are selected/changed\n",
    "                    file_input.change(\n",
    "                        fn=auto_ingest_wrapper,\n",
    "                        inputs=file_input,\n",
    "                        outputs=status_box\n",
    "                    )\n",
    "\n",
    "                with gr.Column(scale=3):\n",
    "                    chatbot = gr.ChatInterface(\n",
    "                        fn=streaming_chat,\n",
    "                        fill_height=True,\n",
    "                        examples=[\"Summarize all uploaded contracts.\", \"What are the common payment terms?\"]\n",
    "                    )\n",
    "\n",
    "        # TAB 2: Analysis Dashboard (Restored)\n",
    "        with gr.Tab(\"ğŸ“Š Analysis & Export\"):\n",
    "            gr.Markdown(\"### ğŸ› ï¸ Session Analysis Tools\")\n",
    "            with gr.Row():\n",
    "                stats_btn = gr.Button(\"ğŸ“Š View Session Stats\", variant=\"secondary\")\n",
    "                eval_btn = gr.Button(\"ğŸ§ª Run Quality Eval\", variant=\"primary\")\n",
    "                export_btn = gr.Button(\"ğŸ“¥ Export Knowledge (JSON)\", variant=\"secondary\")\n",
    "            \n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"Analysis Results\",\n",
    "                interactive=False,\n",
    "                lines=20\n",
    "            )\n",
    "\n",
    "            stats_btn.click(get_session_stats, outputs=analysis_output)\n",
    "            eval_btn.click(run_sample_evaluation, outputs=analysis_output)\n",
    "            export_btn.click(export_knowledge, outputs=analysis_output)\n",
    "\n",
    "print(\"âœ… UI Fixed: Multiple files can be selected and ingested automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://localhost:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">  Loaded </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">page</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">(s) from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"contract_test.pdf\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m  Loaded \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;46;204;113m \u001b[0m\u001b[1;35mpage\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113ms\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m from \u001b[0m\u001b[32m\"contract_test.pdf\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Ingested </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> chunks from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"contract_test.pdf\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Ingested \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;46;204;113m chunks from \u001b[0m\u001b[32m\"contract_test.pdf\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">  Loaded </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">page</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">(s) from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"contract_test.pdf\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m  Loaded \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;46;204;113m \u001b[0m\u001b[1;35mpage\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113ms\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m from \u001b[0m\u001b[32m\"contract_test.pdf\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Ingested </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> chunks from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"contract_test.pdf\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Ingested \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;46;204;113m chunks from \u001b[0m\u001b[32m\"contract_test.pdf\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">  Loaded </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">page</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">(s) from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Ù…Ø«Ø§Ù„ Ø¹Ù‚Ø¯.docx\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113m  Loaded \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;46;204;113m \u001b[0m\u001b[1;35mpage\u001b[0m\u001b[1;38;2;46;204;113m(\u001b[0m\u001b[1;38;2;46;204;113ms\u001b[0m\u001b[1;38;2;46;204;113m)\u001b[0m\u001b[1;38;2;46;204;113m from \u001b[0m\u001b[32m\"Ù…Ø«Ø§Ù„ Ø¹Ù‚Ø¯.docx\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\">âœ… Ingested </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #2ecc71; text-decoration-color: #2ecc71; font-weight: bold\"> chunks from </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Ù…Ø«Ø§Ù„ Ø¹Ù‚Ø¯.docx\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;46;204;113mâœ… Ingested \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;46;204;113m chunks from \u001b[0m\u001b[32m\"Ù…Ø«Ø§Ù„ Ø¹Ù‚Ø¯.docx\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 4.6: Launch Server ------------------------------------------------\n",
    "gr.close_all()\n",
    "\n",
    "try:\n",
    "    app.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "app.launch(\n",
    "    debug=True,\n",
    "    share=False,\n",
    "    server_name=\"localhost\",\n",
    "    css=custom_css,\n",
    "    theme=gr.themes.Soft(primary_hue='emerald')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
